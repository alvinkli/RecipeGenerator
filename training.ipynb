{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.04M/1.04M [00:00<00:00, 6.20MB/s]\n",
      "c:\\Users\\drewj\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\drewj\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 4.63MB/s]\n",
      "Downloading: 100%|██████████| 665/665 [00:00<00:00, 111kB/s]\n",
      "Downloading: 100%|██████████| 548M/548M [00:37<00:00, 14.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "# load in pretrained GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # add in start and end tokens\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').cuda()\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare and load dataset\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train model\n",
    "\"\"\"\n",
    "def train_model(\n",
    "    dataset,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch_size=16,\n",
    "    epochs=5,\n",
    "    lr=2e-5,\n",
    "    max_seq_len=400,\n",
    "    warmup_steps=200,\n",
    "    gpt2_type='gpt2',\n",
    "    output_dir='',\n",
    "    output_prefix='wreckgar',\n",
    "    test_mode=False,\n",
    "    save_model_on_epoch=False):\n",
    "\n",
    "    # define the device\n",
    "    device=torch.device('cuda')\n",
    "\n",
    "    # pass model to deivce\n",
    "    model=model.cuda()\n",
    "\n",
    "    # set model up for training\n",
    "    model.train()\n",
    "\n",
    "    # define optimizer\n",
    "    opt = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # create schedule with linear lr decay from init -> 0 \n",
    "    sched = get_linear_schedule_with_warmup(opt,num_training_steps=warmup_steps,num_warmup_steps=-1)\n",
    "\n",
    "    # create DataLoader to retrieve features and labels from dataset\n",
    "    # use to reshuffle daya at every epoch to avoid overfitting\n",
    "    dataload=DataLoader(dataset, batch_size=1,shuffle=True)\n",
    "\n",
    "    loss=0\n",
    "    batch_count=0\n",
    "\n",
    "    # main training loop\n",
    "    for e in range(epochs):\n",
    "\n",
    "        print(f'Training epoch {e}')\n",
    "        print('loss: ',loss)\n",
    "\n",
    "        for i,inp in tqdm(enumerate(dataload)):\n",
    "\n",
    "            # pass inp to cuda\n",
    "            inp=inp.to(device)\n",
    "\n",
    "            # get model outputs\n",
    "            out=model(inp,labels=inp)\n",
    "\n",
    "            #get loss and backprop\n",
    "            loss=out[0]\n",
    "            loss.backward()\n",
    "\n",
    "            # update every so often\n",
    "            if batch_count % batch_size==0:\n",
    "                opt.step()\n",
    "                sched.step()\n",
    "                opt.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            # update batch count\n",
    "            batch_count+=1\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model=train_model(dataset, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20760/1754071867.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m#Run the functions to generate the lyrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m \u001b[0mgenerated_lyrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_set' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate Recipes\n",
    "\"\"\"\n",
    "\n",
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=30, #maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "              output_list = list(generated.squeeze().numpy())\n",
    "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
    "              generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list\n",
    "\n",
    "#Function to generate multiple sentences. Test data should be a dataframe\n",
    "def text_generation(test_data):\n",
    "  generated_lyrics = []\n",
    "  for i in range(len(test_data)):\n",
    "    x = generate(model.to('cpu'), tokenizer, test_data['Lyric'][i], entry_count=1)\n",
    "    generated_lyrics.append(x)\n",
    "  return generated_lyrics\n",
    "\n",
    "#Run the functions to generate the lyrics\n",
    "generated_lyrics = text_generation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Performance Evaluation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae9b42876be7716e48a57f9874312e4d7d4ba3c63552faac6e21c37780863ecd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
